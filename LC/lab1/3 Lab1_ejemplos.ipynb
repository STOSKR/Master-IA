{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD5taTXsfYzG",
    "nbpresent": {
     "id": "4c3ba838-f644-4890-a61f-0d6c8088fb87"
    }
   },
   "source": [
    "# Ejemplo 1: concordancias, mostrar en qué líneas aparecen las palabras de un texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mqPp63dfYzN",
    "nbpresent": {
     "id": "0e38d85b-82a2-4529-87e0-da681ce47920"
    }
   },
   "source": [
    "### Código python para mostrar el contenido del fichero BowieHeroes.txt (se han numerado las líneas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f42uculOfYzN",
    "nbpresent": {
     "id": "534eb058-559a-450b-b382-e389a0f663ac"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 I, I wish you could swim\n",
      "\n",
      "2 Like the dolphins, like dolphins can swim\n",
      "\n",
      "3 Though nothing, nothing will keep us together\n",
      "\n",
      "4 We can beat them, for ever and ever\n",
      "\n",
      "5 Oh we can be heroes, just for one day\n",
      "\n",
      "6 \n",
      "\n",
      "7 I, I will be king\n",
      "\n",
      "8 And you, you will be queen\n",
      "\n",
      "9 Though nothing will drive them away\n",
      "\n",
      "10 We can be heroes, just for one day\n",
      "\n",
      "11 We can be us, just for one day\n",
      "\n",
      "12 \n",
      "\n",
      "13 I, I can remember (I remember)\n",
      "\n",
      "14 Standing, by the wall (by the wall)\n",
      "\n",
      "15 And the guns, shot above our heads (over our heads)\n",
      "\n",
      "16 And we kissed, as though nothing could fall (nothing could fall)\n",
      "\n",
      "17 And the shame, was on the other side\n",
      "\n",
      "18 Oh, we can beat them, for ever and ever\n",
      "\n",
      "19 Then we could be heroes, just for one day\n",
      "\n",
      "20 \n",
      "\n",
      "21 We can be heroes\n",
      "\n",
      "22 We can be heroes\n",
      "\n",
      "23 We can be heroes\n",
      "\n",
      "24 Just for one day\n",
      "\n",
      "25 We can be heroes\n"
     ]
    }
   ],
   "source": [
    "nol = 1\n",
    "for linea in open('BowieHeroes.txt','r'):\n",
    "    print(\"%d %s\" %(nol, linea))\n",
    "    nol = nol +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8En2DzgfYzO",
    "nbpresent": {
     "id": "a99392c5-6c0c-41d3-b1be-520500d17fe9"
    }
   },
   "source": [
    "### El siguiente código muestra por pantalla un listado con cada palabra, junto con los números de líneas en los que aparece.\n",
    "\n",
    "### - Utilizando `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jdl5-BOpfYzP",
    "nbpresent": {
     "id": "ab1a0278-8fa7-4882-bc5a-5048ceec1bc3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Palabra \t #lineas\n",
      "        i, \t [1, 7, 13]\n",
      "         i \t [1, 7, 13]\n",
      "      wish \t [1]\n",
      "       you \t [1, 8]\n",
      "     could \t [1, 16, 16, 19]\n",
      "      swim \t [1, 2]\n",
      "      like \t [2, 2]\n",
      "       the \t [2, 14, 14, 15, 17, 17]\n",
      " dolphins, \t [2]\n",
      "  dolphins \t [2]\n",
      "       can \t [2, 4, 5, 10, 11, 13, 18, 21, 22, 23, 25]\n",
      "    though \t [3, 9, 16]\n",
      "  nothing, \t [3]\n",
      "   nothing \t [3, 9, 16]\n",
      "      will \t [3, 7, 8, 9]\n",
      "      keep \t [3]\n",
      "        us \t [3]\n",
      "  together \t [3]\n",
      "        we \t [4, 5, 10, 11, 16, 18, 19, 21, 22, 23, 25]\n",
      "      beat \t [4, 18]\n",
      "     them, \t [4, 18]\n",
      "       for \t [4, 5, 10, 11, 18, 19, 24]\n",
      "      ever \t [4, 4, 18, 18]\n",
      "       and \t [4, 8, 15, 16, 17, 18]\n",
      "        oh \t [5]\n",
      "        be \t [5, 7, 8, 10, 11, 19, 21, 22, 23, 25]\n",
      "   heroes, \t [5, 10, 19]\n",
      "      just \t [5, 10, 11, 19, 24]\n",
      "       one \t [5, 10, 11, 19, 24]\n",
      "       day \t [5, 10, 11, 19, 24]\n",
      "      king \t [7]\n",
      "      you, \t [8]\n",
      "     queen \t [8]\n",
      "     drive \t [9]\n",
      "      them \t [9]\n",
      "      away \t [9]\n",
      "       us, \t [11]\n",
      "  remember \t [13]\n",
      "        (i \t [13]\n",
      " remember) \t [13]\n",
      " standing, \t [14]\n",
      "        by \t [14]\n",
      "      wall \t [14]\n",
      "       (by \t [14]\n",
      "     wall) \t [14]\n",
      "     guns, \t [15]\n",
      "      shot \t [15]\n",
      "     above \t [15]\n",
      "       our \t [15, 15]\n",
      "     heads \t [15]\n",
      "     (over \t [15]\n",
      "    heads) \t [15]\n",
      "   kissed, \t [16]\n",
      "        as \t [16]\n",
      "      fall \t [16]\n",
      "  (nothing \t [16]\n",
      "     fall) \t [16]\n",
      "    shame, \t [17]\n",
      "       was \t [17]\n",
      "        on \t [17]\n",
      "     other \t [17]\n",
      "      side \t [17]\n",
      "       oh, \t [18]\n",
      "      then \t [19]\n",
      "    heroes \t [21, 22, 23, 25]\n"
     ]
    }
   ],
   "source": [
    "dic = {}       ## diccionario: claves palabras - valores lista de # lineas\n",
    "cont = 1       ## contador de lineas\n",
    "for linea in open('BowieHeroes.txt','r'):\n",
    "   for pal in linea.lower().split():\n",
    "     if pal in dic:\n",
    "       dic[pal].append(cont)\n",
    "     else: dic[pal] = [cont]\n",
    "   cont += 1\n",
    "\n",
    "## listado\n",
    "print(\"%10s \\t %s\" %(\"Palabra\", \"#lineas\"))\n",
    "for k,v in dic.items():\n",
    "   print(\"%10s \\t %s\" %(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "cmJ4gWjlfYzP"
   },
   "source": [
    "\n",
    "### - Utilizando el tokenizador de nltk - `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ADjgTNVwjVGx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shiyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4nH9BYa8fYzP",
    "nbpresent": {
     "id": "de8da9d5-78ea-4019-b1dd-9087453c2fc8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Palabra \t #lineas\n",
      "         i \t [1, 1, 7, 7, 13, 13, 13]\n",
      "         , \t [1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 18, 19]\n",
      "      wish \t [1]\n",
      "       you \t [1, 8, 8]\n",
      "     could \t [1, 16, 16, 19]\n",
      "      swim \t [1, 2]\n",
      "      like \t [2, 2]\n",
      "       the \t [2, 14, 14, 15, 17, 17]\n",
      "  dolphins \t [2, 2]\n",
      "       can \t [2, 4, 5, 10, 11, 13, 18, 21, 22, 23, 25]\n",
      "    though \t [3, 9, 16]\n",
      "   nothing \t [3, 3, 9, 16, 16]\n",
      "      will \t [3, 7, 8, 9]\n",
      "      keep \t [3]\n",
      "        us \t [3, 11]\n",
      "  together \t [3]\n",
      "        we \t [4, 5, 10, 11, 16, 18, 19, 21, 22, 23, 25]\n",
      "      beat \t [4, 18]\n",
      "      them \t [4, 9, 18]\n",
      "       for \t [4, 5, 10, 11, 18, 19, 24]\n",
      "      ever \t [4, 4, 18, 18]\n",
      "       and \t [4, 8, 15, 16, 17, 18]\n",
      "        oh \t [5, 18]\n",
      "        be \t [5, 7, 8, 10, 11, 19, 21, 22, 23, 25]\n",
      "    heroes \t [5, 10, 19, 21, 22, 23, 25]\n",
      "      just \t [5, 10, 11, 19, 24]\n",
      "       one \t [5, 10, 11, 19, 24]\n",
      "       day \t [5, 10, 11, 19, 24]\n",
      "      king \t [7]\n",
      "     queen \t [8]\n",
      "     drive \t [9]\n",
      "      away \t [9]\n",
      "  remember \t [13, 13]\n",
      "         ( \t [13, 14, 15, 16]\n",
      "         ) \t [13, 14, 15, 16]\n",
      "  standing \t [14]\n",
      "        by \t [14, 14]\n",
      "      wall \t [14, 14]\n",
      "      guns \t [15]\n",
      "      shot \t [15]\n",
      "     above \t [15]\n",
      "       our \t [15, 15]\n",
      "     heads \t [15, 15]\n",
      "      over \t [15]\n",
      "    kissed \t [16]\n",
      "        as \t [16]\n",
      "      fall \t [16, 16]\n",
      "     shame \t [17]\n",
      "       was \t [17]\n",
      "        on \t [17]\n",
      "     other \t [17]\n",
      "      side \t [17]\n",
      "      then \t [19]\n"
     ]
    }
   ],
   "source": [
    "dic = {}       ## diccionario: claves palabras - valores lista de # lineas\n",
    "cont = 1       ## contador de lineas\n",
    "for linea in open('BowieHeroes.txt','r'):\n",
    "   for pal in word_tokenize(linea.lower()):\n",
    "     if pal in dic:\n",
    "       dic[pal].append(cont)\n",
    "     else: dic[pal] = [cont]\n",
    "   cont += 1\n",
    "\n",
    "## listado\n",
    "print(\"%10s \\t %s\" %(\"Palabra\", \"#lineas\"))\n",
    "for k,v in dic.items():\n",
    "   print(\"%10s \\t %s\" %(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEoRtrIKR9uX"
   },
   "source": [
    "# Ejemplo 2: Segmentación en oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioTCrSk-j0WP"
   },
   "source": [
    "### `sent_tokenize` por defecto segmenta textos en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YgtrmYuMSvlM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 0: U.S.A. is a big country.\n",
      "tokens:  ['U.S.A.', 'is', 'a', 'big', 'country', '.']\n",
      "Oración 1: More than 1.000.000 people live in it!\n",
      "tokens:  ['More', 'than', '1.000.000', 'people', 'live', 'in', 'it', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "j=0\n",
    "for i in sent_tokenize(\"U.S.A. is a big country. More than 1.000.000 people live in it!\"):\n",
    "  print (\"Oración %s: %s\" %(j, i))\n",
    "  j=j+1\n",
    "  print(\"tokens: \", word_tokenize(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQgWhB-hk6sv"
   },
   "source": [
    "### Con el parámetro `language` podemos cambiar el idioma de `sent_tokenize` y de `word_tokenize`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IYTs8m51S0wL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 2: El Dr. Pérez vendrá a las 18:30.\n",
      "tokens:  ['El', 'Dr.', 'Pérez', 'vendrá', 'a', 'las', '18:30', '.']\n",
      "Oración 1: ¡A ver si nos atiende rápido!\n",
      "tokens:  ['¡A', 'ver', 'si', 'nos', 'atiende', 'rápido', '!']\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokenize(\"El Dr. Pérez vendrá a las 18:30. ¡A ver si nos atiende rápido!\",language='spanish'):\n",
    "  print (\"Oración %s: %s\" %(j, i))\n",
    "  j=+1\n",
    "  print(\"tokens: \", word_tokenize(i,language='spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO0qxlcmle2H"
   },
   "source": [
    "### Podemos crear nuestro propio tokenizador basado en expresiones regulares con `regexp_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z_4Vj0nfS3c_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', 'Hola', '!', '¿', 'Cómo', 'estáis', '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "regexp_tokenize(\"!Hola! ¿Cómo estáis?\",r\"([\\w]+|[¡!?¿])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1otHRG8c4mH3"
   },
   "source": [
    "# Ejemplo 3: Análisis de Macbeth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4ZFh2-m4mH8"
   },
   "source": [
    "### Analiza el texto Macbeth de Shakespeare, que está en el corpus gutenberg de nltk.corpus (shakespeare-macbeth.txt). A continuación se escriben instrucciones python para:\n",
    "\n",
    "###3.1.- Listar los ficheros del corpus gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ILz-882dU-uB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\shiyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "#nltk.download('punkt_tab')\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RBdHyzal4mH-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Ficheros del corpus gutenberg:\n",
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"... Ficheros del corpus gutenberg:\")\n",
    "for fileid in (nltk.corpus.gutenberg.fileids()):\n",
    "    print(fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi-_FAGu4mH-"
   },
   "source": [
    "###3.2.- Función que lista, para el corpus que se indica, las longitudes medias de sus palabras (lmWord) y frases (lmSent) y la frecuencia de uso media de cada palabra (diversidad léxica - lds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "ij6AFFzg4mH_"
   },
   "outputs": [],
   "source": [
    "def getCorpusInfo(corpus):\n",
    "    print(\"... Mostrando información de los ficheros del corpus \", corpus)\n",
    "    print(\"%s %s %s %s\" %(\"lmWord\", \"lmSent\", \"divLex\", \"texto\"))\n",
    "    for fileid in corpus.fileids():\n",
    "        noCars = len(corpus.raw(fileid))\n",
    "        noWords = len(corpus.words(fileid))\n",
    "        noSents = len(corpus.sents(fileid))\n",
    "        vocab = len(set(w.lower() for w in corpus.words(fileid)))\n",
    "        lmWord = round(noCars / noWords)\n",
    "        lmSent = round(noWords / noSents)\n",
    "        lds = round(noWords / vocab)\n",
    "        print(\"%4d %6d %6d   %s\" %(lmWord, lmSent, lds, fileid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIcSmT64mH_"
   },
   "source": [
    "###3.3.- Utilizar la función anterior para obtener información sobre el corpus gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SznYUaY34mH_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Mostrando información de los ficheros del corpus  <PlaintextCorpusReader in 'C:\\\\Users\\\\shiyi\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\gutenberg'>\n",
      "lmWord lmSent divLex texto\n",
      "   5     25     26   austen-emma.txt\n",
      "   5     26     17   austen-persuasion.txt\n",
      "   5     28     22   austen-sense.txt\n",
      "   4     34     79   bible-kjv.txt\n",
      "   5     19      5   blake-poems.txt\n",
      "   4     19     14   bryant-stories.txt\n",
      "   4     18     12   burgess-busterbrown.txt\n",
      "   4     20     13   carroll-alice.txt\n",
      "   5     20     12   chesterton-ball.txt\n",
      "   5     23     11   chesterton-brown.txt\n",
      "   5     18     11   chesterton-thursday.txt\n",
      "   4     21     25   edgeworth-parents.txt\n",
      "   5     26     15   melville-moby_dick.txt\n",
      "   5     52     11   milton-paradise.txt\n",
      "   4     12      9   shakespeare-caesar.txt\n",
      "   4     12      8   shakespeare-hamlet.txt\n",
      "   4     12      7   shakespeare-macbeth.txt\n",
      "   5     36     12   whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "getCorpusInfo(nltk.corpus.gutenberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUFgOU8e4mIA"
   },
   "source": [
    "### 3.4.- Obtener las frecuencias de aparición de las palabras del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "F_omnD_y4mIA"
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(word.lower() for word in gutenberg.words('shakespeare-macbeth.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4KFEBoV4mIA"
   },
   "source": [
    "### 3.5.- Mostrar la palabra mas frecuente y su probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uYqFym7z4mIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... La palabra más frecuente es:  ,\n",
      "... y su probabilidad es:  0.08478824546240277\n"
     ]
    }
   ],
   "source": [
    "print(\"... La palabra más frecuente es: \", fdist.max())\n",
    "print(\"... y su probabilidad es: \", fdist.freq(fdist.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLXIJ9ZG4mIB"
   },
   "source": [
    "###3.6.- Mostrar las 20 palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CVFdcZSd4mIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Las 20 palabras más frecuentes en Macbeth son:\n",
      "FreqDist({',': 1962, '.': 1235, 'the': 650, \"'\": 637, 'and': 546, ':': 477, 'to': 384, 'i': 348, 'of': 338, '?': 241, 'a': 241, 'that': 238, 'd': 224, 'you': 206, 'my': 203, 'in': 201, 'is': 188, 'not': 165, 'it': 161, 'with': 153, ...})\n"
     ]
    }
   ],
   "source": [
    "print(\"... Las 20 palabras más frecuentes en Macbeth son:\")\n",
    "fdist.pprint(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kWZuILg4mIB"
   },
   "source": [
    "###3.7.- Mostrar las frecuencias de aparición y probabilidades de las palabras siguientes:\n",
    "'Duncan':  rey de Escocia\n",
    "'Macbeth' y 'Banquo': generales del ejercito escocés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jc7aL46-4mIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... duncan aparece 10 veces\n",
      "... macbeth aparece 62 veces\n",
      "... banquo aparece 39 veces\n"
     ]
    }
   ],
   "source": [
    "print(\"... duncan aparece %d veces\" % (fdist['duncan']))\n",
    "print(\"... macbeth aparece %d veces\" % (fdist['macbeth']))\n",
    "print(\"... banquo aparece %d veces\" % (fdist['banquo']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAY8fnr-4mIC"
   },
   "source": [
    "###3.8.- Mostrar el tamaño del léxico utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LXH1w0DF4mIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... El tamaño del léxico de Macbeth es:  3464\n"
     ]
    }
   ],
   "source": [
    "vocab = fdist.keys()\n",
    "print(\"... El tamaño del léxico de Macbeth es: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZjFkVAe4mIC"
   },
   "source": [
    "### 3.9.- Mostrar cuántas palabras aparecen una sóla vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RWHv-Woc4mIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... El número de palabras que aparecen una sola vez es:  2082\n"
     ]
    }
   ],
   "source": [
    "raras = fdist.hapaxes()\n",
    "print(\"... El número de palabras que aparecen una sola vez es: \", len(raras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C57h2Yg74mIC"
   },
   "source": [
    "###3.10.- De las palabras que aparecen una sóla vez, listar 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zH_MwNih4mIC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Algunas de ellas son:\n",
      "['william', 'shakespeare', '1603', 'primus', 'scoena', 'raine', 'hurley', 'burley', 'battaile', 'gray', 'malkin', 'padock', 'houer', 'fogge', 'malcome', 'captaine', 'seemeth', 'plight', 'serieant', 'hardie', 'captiuitie', 'broyle', 'swimmers', 'choake', 'mercilesse', 'macdonwald', 'worthie', 'multiplying', 'villanies', 'swarme', 'westerne', 'isles', 'gallowgrosses', 'supply', 'smiling', 'rebells', 'whore', 'disdayning', 'brandisht', 'smoak', 'valours', 'minion', 'caru', 'neu', 'r', 'shooke', 'vnseam', 'naue', 'chops', 'fix']\n"
     ]
    }
   ],
   "source": [
    "print(\"... Algunas de ellas son:\")\n",
    "print(raras[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYzWqrBwXMdz"
   },
   "source": [
    "# Ejemplo 4: eliminar **stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p7Z0T8smcTb"
   },
   "source": [
    "Las *stopwords* son palabras sin carga semántica o muy comunes, que pueden eliminarse para si se considera que no son relevantes para la tarea a resolver (sistemas de recuperación de información) o si su eliminación puede mejorar el rendimiento (tareas de clasificación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SgzyTSDcK6Xe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shiyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['albanian',\n",
       " 'arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'belarusian',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'tamil',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aUXWQ3Ch5zhx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('spanish'))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UbsMZbtYbpgA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I, I wish you could swim\n",
      "\n",
      "Simplificada: , wish could swim\n",
      "\n",
      "Original: Like the dolphins, like dolphins can swim\n",
      "\n",
      "Simplificada: Like dolphins , like dolphins swim\n",
      "\n",
      "Original: Though nothing, nothing will keep us together\n",
      "\n",
      "Simplificada: Though nothing , nothing keep us together\n",
      "\n",
      "Original: We can beat them, for ever and ever\n",
      "\n",
      "Simplificada: beat , ever ever\n",
      "\n",
      "Original: Oh we can be heroes, just for one day\n",
      "\n",
      "Simplificada: Oh heroes , one day\n",
      "\n",
      "Original: \n",
      "\n",
      "Simplificada: \n",
      "\n",
      "Original: I, I will be king\n",
      "\n",
      "Simplificada: , king\n",
      "\n",
      "Original: And you, you will be queen\n",
      "\n",
      "Simplificada: , queen\n",
      "\n",
      "Original: Though nothing will drive them away\n",
      "\n",
      "Simplificada: Though nothing drive away\n",
      "\n",
      "Original: We can be heroes, just for one day\n",
      "\n",
      "Simplificada: heroes , one day\n",
      "\n",
      "Original: We can be us, just for one day\n",
      "\n",
      "Simplificada: us , one day\n",
      "\n",
      "Original: \n",
      "\n",
      "Simplificada: \n",
      "\n",
      "Original: I, I can remember (I remember)\n",
      "\n",
      "Simplificada: , remember ( remember )\n",
      "\n",
      "Original: Standing, by the wall (by the wall)\n",
      "\n",
      "Simplificada: Standing , wall ( wall )\n",
      "\n",
      "Original: And the guns, shot above our heads (over our heads)\n",
      "\n",
      "Simplificada: guns , shot heads ( heads )\n",
      "\n",
      "Original: And we kissed, as though nothing could fall (nothing could fall)\n",
      "\n",
      "Simplificada: kissed , though nothing could fall ( nothing could fall )\n",
      "\n",
      "Original: And the shame, was on the other side\n",
      "\n",
      "Simplificada: shame , side\n",
      "\n",
      "Original: Oh, we can beat them, for ever and ever\n",
      "\n",
      "Simplificada: Oh , beat , ever ever\n",
      "\n",
      "Original: Then we could be heroes, just for one day\n",
      "\n",
      "Simplificada: could heroes , one day\n",
      "\n",
      "Original: \n",
      "\n",
      "Simplificada: \n",
      "\n",
      "Original: We can be heroes\n",
      "\n",
      "Simplificada: heroes\n",
      "\n",
      "Original: We can be heroes\n",
      "\n",
      "Simplificada: heroes\n",
      "\n",
      "Original: We can be heroes\n",
      "\n",
      "Simplificada: heroes\n",
      "\n",
      "Original: Just for one day\n",
      "\n",
      "Simplificada: one day\n",
      "\n",
      "Original: We can be heroes\n",
      "Simplificada: heroes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for linea in open('BowieHeroes.txt','r'):\n",
    "    pal_no_sw= [pal for pal in word_tokenize(linea)\n",
    "       if pal.lower() not in stopwords.words('english')]\n",
    "    print(\"Original: \" + linea)\n",
    "    print(\"Simplificada: \" + \" \".join(pal_no_sw) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nKArPCiC04J7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivÃ­a un hidalgo de los de lanza en astillero, adarga antigua, rocÃ­n flaco y galgo corredor.\n",
      "Simplificada: lugar Mancha , cuyo nombre quiero acordarme , tiempo vivÃ­a hidalgo lanza astillero , adarga antigua , rocÃ­n flaco galgo corredor .\n",
      "\n",
      "Original: Una olla de algo mÃ¡s vaca que carnero, salpicÃ³n las mÃ¡s noches, duelos y quebrantos los sÃ¡bados, lantejas los viernes, algÃºn palomino de aÃ±adidura los domingos, consumÃ­an las tres cuartas partes de su hacienda.\n",
      "Simplificada: olla mÃ¡s vaca carnero , salpicÃ³n mÃ¡s noches , duelos quebrantos sÃ¡bados , lantejas viernes , algÃºn palomino aÃ±adidura domingos , consumÃ­an tres cuartas partes hacienda .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texto = open('quijote.txt','rt').read()\n",
    "for linea in sent_tokenize(texto,language='spanish'):\n",
    "    pal_no_sw= [pal for pal in word_tokenize(linea)\n",
    "       if pal.lower() not in stopwords.words('spanish')]\n",
    "    print(\"Original: \" + linea)\n",
    "    print(\"Simplificada: \" + \" \".join(pal_no_sw) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU7WdCvNvAdj"
   },
   "source": [
    "# Ejemplo 5: ***Stemming***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4WyVxHxkyXL"
   },
   "source": [
    "El *stemming* consiste en truncar las palabras eliminando los afijos y dejando únicamente la raíz (*stem*) aplicando una secuencia de reglas. P.e. el *stem* de cooking, cooked y cooks es cook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1GE_pbd5ff_Z"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td89zV2Hfx73"
   },
   "source": [
    "### `PorterStemmer()` es un *stemmer* para el inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PZGtIEoXITP4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I i\n",
      ", ,\n",
      "I i\n",
      "wish wish\n",
      "you you\n",
      "could could\n",
      "swim swim\n",
      "Like like\n",
      "the the\n",
      "dolphins dolphin\n",
      ", ,\n",
      "like like\n",
      "dolphins dolphin\n",
      "can can\n",
      "swim swim\n",
      "Though though\n",
      "nothing noth\n",
      ", ,\n",
      "nothing noth\n",
      "will will\n",
      "keep keep\n",
      "us us\n",
      "together togeth\n",
      "We we\n",
      "can can\n",
      "beat beat\n",
      "them them\n",
      ", ,\n",
      "for for\n",
      "ever ever\n",
      "and and\n",
      "ever ever\n",
      "Oh oh\n",
      "we we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "I i\n",
      ", ,\n",
      "I i\n",
      "will will\n",
      "be be\n",
      "king king\n",
      "And and\n",
      "you you\n",
      ", ,\n",
      "you you\n",
      "will will\n",
      "be be\n",
      "queen queen\n",
      "Though though\n",
      "nothing noth\n",
      "will will\n",
      "drive drive\n",
      "them them\n",
      "away away\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "us us\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "I i\n",
      ", ,\n",
      "I i\n",
      "can can\n",
      "remember rememb\n",
      "( (\n",
      "I i\n",
      "remember rememb\n",
      ") )\n",
      "Standing stand\n",
      ", ,\n",
      "by by\n",
      "the the\n",
      "wall wall\n",
      "( (\n",
      "by by\n",
      "the the\n",
      "wall wall\n",
      ") )\n",
      "And and\n",
      "the the\n",
      "guns gun\n",
      ", ,\n",
      "shot shot\n",
      "above abov\n",
      "our our\n",
      "heads head\n",
      "( (\n",
      "over over\n",
      "our our\n",
      "heads head\n",
      ") )\n",
      "And and\n",
      "we we\n",
      "kissed kiss\n",
      ", ,\n",
      "as as\n",
      "though though\n",
      "nothing noth\n",
      "could could\n",
      "fall fall\n",
      "( (\n",
      "nothing noth\n",
      "could could\n",
      "fall fall\n",
      ") )\n",
      "And and\n",
      "the the\n",
      "shame shame\n",
      ", ,\n",
      "was wa\n",
      "on on\n",
      "the the\n",
      "other other\n",
      "side side\n",
      "Oh oh\n",
      ", ,\n",
      "we we\n",
      "can can\n",
      "beat beat\n",
      "them them\n",
      ", ,\n",
      "for for\n",
      "ever ever\n",
      "and and\n",
      "ever ever\n",
      "Then then\n",
      "we we\n",
      "could could\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "Just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "for linea in open('BowieHeroes.txt','r'):\n",
    "    linea2=''\n",
    "    for pal in word_tokenize(linea):\n",
    "        print (pal +\" \" +stemmer.stem(pal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCkHG2BBlTPl"
   },
   "source": [
    "### `SnowballStemmer()` es un *stemmer* para varios idiomas, entre ellos el español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CYDB7hTgNKdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En en\n",
      "un un\n",
      "lugar lug\n",
      "de de\n",
      "la la\n",
      "Mancha manch\n",
      ", ,\n",
      "de de\n",
      "cuyo cuy\n",
      "nombre nombr\n",
      "no no\n",
      "quiero quier\n",
      "acordarme acord\n",
      ", ,\n",
      "no no\n",
      "ha ha\n",
      "mucho much\n",
      "tiempo tiemp\n",
      "que que\n",
      "vivÃ­a vivã­\n",
      "un un\n",
      "hidalgo hidalg\n",
      "de de\n",
      "los los\n",
      "de de\n",
      "lanza lanz\n",
      "en en\n",
      "astillero astiller\n",
      ", ,\n",
      "adarga adarg\n",
      "antigua antigu\n",
      ", ,\n",
      "rocÃ­n rocã­n\n",
      "flaco flac\n",
      "y y\n",
      "galgo galg\n",
      "corredor corredor\n",
      ". .\n",
      "Una una\n",
      "olla olla\n",
      "de de\n",
      "algo algo\n",
      "mÃ¡s mã¡s\n",
      "vaca vac\n",
      "que que\n",
      "carnero carner\n",
      ", ,\n",
      "salpicÃ³n salpicã³n\n",
      "las las\n",
      "mÃ¡s mã¡s\n",
      "noches noch\n",
      ", ,\n",
      "duelos duel\n",
      "y y\n",
      "quebrantos quebrant\n",
      "los los\n",
      "sÃ¡bados sã¡bad\n",
      ", ,\n",
      "lantejas lantej\n",
      "los los\n",
      "viernes viern\n",
      ", ,\n",
      "algÃºn algãºn\n",
      "palomino palomin\n",
      "de de\n",
      "aÃ±adidura aã±adidur\n",
      "los los\n",
      "domingos doming\n",
      ", ,\n",
      "consumÃ­an consumã­\n",
      "las las\n",
      "tres tres\n",
      "cuartas cuart\n",
      "partes part\n",
      "de de\n",
      "su su\n",
      "hacienda haciend\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "stemmer=SnowballStemmer('spanish')\n",
    "texto = open('quijote.txt','rt').read()\n",
    "for linea in sent_tokenize(texto,language='spanish'):\n",
    "    linea2=''\n",
    "    for pal in word_tokenize(linea):\n",
    "      print (pal +\" \" +stemmer.stem(pal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "rrwlC5TJOz5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quer\n",
      "quer\n",
      "quier\n",
      "gatit\n",
      "gat\n",
      "com\n",
      "com\n",
      "com\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#no maneja las irregularidades de idomas como el español\n",
    "#funciona mejor con lenguajes poco flexivos como el inglés\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer=SnowballStemmer('spanish')\n",
    "\n",
    "print(stemmer.stem('querido'))\n",
    "print(stemmer.stem('querer'))\n",
    "print(stemmer.stem('quiero'))\n",
    "\n",
    "print(stemmer.stem('gatito'))\n",
    "print(stemmer.stem('gato'))\n",
    "\n",
    "print(stemmer.stem('comido'))\n",
    "print(stemmer.stem('comer'))\n",
    "print(stemmer.stem('comía'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0rCA07dvYVP"
   },
   "source": [
    "\n",
    "# Ejemplo 6: lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkTpKsqRnTIf"
   },
   "source": [
    "Un lematizador devuelve la forma base o canónica de las palabras con formas flexivas:\n",
    "\n",
    "*   verbos -> forma en infinitivo\n",
    "*   nombres, adjetivos, determinantes y pronombres -> forma masculino, singular si tienen variabilidad de género y número\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "2sPaGXcxpV4h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shiyi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "id": "PynaSXI3Tu4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      ", ,\n",
      "I I\n",
      "wish wish\n",
      "you you\n",
      "could could\n",
      "swim swim\n",
      "Like Like\n",
      "the the\n",
      "dolphins dolphin\n",
      ", ,\n",
      "like like\n",
      "dolphins dolphin\n",
      "can can\n",
      "swim swim\n",
      "Though Though\n",
      "nothing nothing\n",
      ", ,\n",
      "nothing nothing\n",
      "will will\n",
      "keep keep\n",
      "us u\n",
      "together together\n",
      "We We\n",
      "can can\n",
      "beat beat\n",
      "them them\n",
      ", ,\n",
      "for for\n",
      "ever ever\n",
      "and and\n",
      "ever ever\n",
      "Oh Oh\n",
      "we we\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "I I\n",
      ", ,\n",
      "I I\n",
      "will will\n",
      "be be\n",
      "king king\n",
      "And And\n",
      "you you\n",
      ", ,\n",
      "you you\n",
      "will will\n",
      "be be\n",
      "queen queen\n",
      "Though Though\n",
      "nothing nothing\n",
      "will will\n",
      "drive drive\n",
      "them them\n",
      "away away\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "us u\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "I I\n",
      ", ,\n",
      "I I\n",
      "can can\n",
      "remember remember\n",
      "( (\n",
      "I I\n",
      "remember remember\n",
      ") )\n",
      "Standing Standing\n",
      ", ,\n",
      "by by\n",
      "the the\n",
      "wall wall\n",
      "( (\n",
      "by by\n",
      "the the\n",
      "wall wall\n",
      ") )\n",
      "And And\n",
      "the the\n",
      "guns gun\n",
      ", ,\n",
      "shot shot\n",
      "above above\n",
      "our our\n",
      "heads head\n",
      "( (\n",
      "over over\n",
      "our our\n",
      "heads head\n",
      ") )\n",
      "And And\n",
      "we we\n",
      "kissed kissed\n",
      ", ,\n",
      "as a\n",
      "though though\n",
      "nothing nothing\n",
      "could could\n",
      "fall fall\n",
      "( (\n",
      "nothing nothing\n",
      "could could\n",
      "fall fall\n",
      ") )\n",
      "And And\n",
      "the the\n",
      "shame shame\n",
      ", ,\n",
      "was wa\n",
      "on on\n",
      "the the\n",
      "other other\n",
      "side side\n",
      "Oh Oh\n",
      ", ,\n",
      "we we\n",
      "can can\n",
      "beat beat\n",
      "them them\n",
      ", ,\n",
      "for for\n",
      "ever ever\n",
      "and and\n",
      "ever ever\n",
      "Then Then\n",
      "we we\n",
      "could could\n",
      "be be\n",
      "heroes hero\n",
      ", ,\n",
      "just just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "heroes hero\n",
      "Just Just\n",
      "for for\n",
      "one one\n",
      "day day\n",
      "We We\n",
      "can can\n",
      "be be\n",
      "heroes hero\n"
     ]
    }
   ],
   "source": [
    "wnl=WordNetLemmatizer()\n",
    "for linea in open('BowieHeroes.txt','r'):\n",
    "    linea2=''\n",
    "    for pal in word_tokenize(linea):\n",
    "        print (pal +\" \" +wnl.lemmatize(pal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFZ6ab5lp_EB"
   },
   "source": [
    "Para lematizar correctamente es necesario conocer la categoría gramatical de la palabra ('v','n','a','r'). Sería necesario ejecutar antes un POS Tagger que proporcionara la categoría gramatical para cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rbkQuKb1o9Z-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'count'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"counted\",\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BqgvqXbYpMDQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'counted'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"counted\",\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cq6IoJDoqY0A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "eSs6OeAvqcTn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"am\",\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIIOQlmssEYx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Ui6RATdiqmZU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'just', 'ate', 'some', 'apples', '.']\n",
      "['i', 'just', 'ate', 'some', 'apple', '.']\n"
     ]
    }
   ],
   "source": [
    "frase=\"I just ate some apples.\"\n",
    "#frase=\"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "tokens=word_tokenize(frase)\n",
    "\n",
    "print(tokens)\n",
    "lemas=[]\n",
    "for t in tokens:\n",
    "    lemas.append(wnl.lemmatize(t.lower()))\n",
    "print(lemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7eNS94Gsg60"
   },
   "source": [
    "## Lematización con spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jUQpmhHUrrwr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      2\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython -m spacy download en_core_web_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the spaCy English model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "#text=\"I just ate some apples.\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract lemmatized tokens\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "# Join the lemmatized tokens into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nbpresent": {
   "slides": {
    "1c3a8d0b-0b7c-4867-88cf-18c42179fede": {
     "id": "1c3a8d0b-0b7c-4867-88cf-18c42179fede",
     "layout": "treemap",
     "prev": null,
     "regions": {
      "b05523f8-8c96-48c0-9893-e4c1ed8a9d23": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "b05523f8-8c96-48c0-9893-e4c1ed8a9d23"
      }
     }
    }
   },
   "themes": {
    "default": "0d06f8c5-fd8a-4657-956b-1abcac8d9e08",
    "theme": {
     "0d06f8c5-fd8a-4657-956b-1abcac8d9e08": {
      "id": "0d06f8c5-fd8a-4657-956b-1abcac8d9e08",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     },
     "857cac3c-9845-463a-82c9-991cb9a5aca1": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "857cac3c-9845-463a-82c9-991cb9a5aca1",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
